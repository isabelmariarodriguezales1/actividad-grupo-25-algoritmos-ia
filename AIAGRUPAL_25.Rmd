---
title: "ACTIVIDAD GRUPAL (ALGORITMOS E INTELIGENCIA ARTIFICIAL) - GRUPO 25"
author: "Isabel María Rodríguez Alés, Julián Soriano Valero, Jaime Hernán Gil, Irene Salas Ibáñez y Júlia Francisco Rodon"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: tango
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(pheatmap)
library(RColorBrewer)

set.seed(123)

library(tidyverse)
library(caret)
library(factoextra)
library(plotly)
library(uwot)
library(ggplot2)
library(randomForest)



```

# 1. INTRODUCCIÓN

En la última década, la biología molecular ha experimentado una transformación sin precedentes gracias al desarrollo de las tecnologías de secuenciación de alto rendimiento (High-Throughput Sequencing). Estas herramientas permiten medir simultáneamente la expresión de miles de genes, generando una cantidad ingente de datos que superan la capacidad de análisis estadístico convencional. En este contexto, la Inteligencia Artificial (IA) y los algoritmos de Machine Learning se han consolidado como pilares fundamentales para la interpretación de sistemas biológicos complejos.

El presente trabajo aborda el análisis de un conjunto de datos de expresión génica compuesto por 801 muestras y 500 variables (genes). Este tipo de datos se caracteriza por el fenómeno conocido como "la maldición de la dimensionalidad" (curse of dimensionality), donde el número de variables es significativamente alto en comparación con el número de observaciones. El reto principal reside en identificar patrones biológicos significativos, conocidos como firmas moleculares, que permitan distinguir entre diferentes tipologías celulares o patologías.

# 2. METODOLOGÍA

Para abordar este desafío, se ha diseñado un flujo de trabajo integral que combina dos vertientes del aprendizaje automático:

-   Aprendizaje No Supervisado: Mediante técnicas de reducción de dimensionalidad y clusterización (como el Análisis de Componentes Principales y la Clusterización Jerárquica), se busca explorar la estructura intrínseca de los datos sin etiquetas previas, facilitando la visualización y el descubrimiento de grupos naturales de muestras.

-   Aprendizaje Supervisado: Utilizando modelos de clasificación (como Random Forest y SVM), se entrena al sistema para reconocer y predecir la pertenencia de una muestra a una clase específica, evaluando la robustez de los modelos mediante métricas de precisión, sensibilidad y especificidad.

A través de la implementación de estos algoritmos en el lenguaje de programación R, este informe pretende demostrar cómo la aplicación rigurosa de la estadística y el aprendizaje automático permite transformar datos genómicos brutos en conocimiento biomédico accionable, facilitando el camino hacia la medicina de precisión y el diagnóstico personalizado.

# 3. PREPROCESAMIENTO

El flujo de trabajo comenzó con la integración de los perfiles de expresión génica, su nomenclatura y etiquetas clínicas. Para garantizar la integridad de los datos, se aplicó un protocolo de control de calidad en etapas sucesivas: en primer lugar, se realizó un filtrado de muestras (filas) para eliminar aquellas con baja calidad técnica basadas en un exceso de valores nulos. Posteriormente, se efectuó un cribado de variables (genes), descartando aquellas con una presencia de ceros superior al 90% y aquellas con varianza nula (SD=0). Estas últimas se eliminan ya que, al ser constantes, no aportan información útil para distinguir patrones; además, su exclusión es matemáticamente necesaria para evitar errores de división por cero durante el escalado y asegurar que algoritmos como PCA o k-means se centren exclusivamente en la variabilidad biológica informativa. Finalmente, se aplicó una normalización mediante centrado y escalado sobre el set de datos depurado para estandarizar las magnitudes y permitir una comparación equitativa entre las variables restantes.

```{r PROCESADO, include=TRUE}


gene_expression <- read.table("gene_expression.csv",
                              sep = ";",
                              header = FALSE)

##carga de nombres de genes
column_names <- read.table("column_names.txt",
                           header = FALSE,
                           stringsAsFactors = FALSE)
##cargar las clases
classes <- read.table("classes.csv",
                      sep = ";",
                      header = FALSE,
                      stringsAsFactors = FALSE)
colnames(classes) <- c("SampleID", "Class")


#CONSTRUIR EL DATAFRAME FINAL

##Asignar nombres de genes como nombres de columnas
colnames(gene_expression) <- column_names$V1
##Asignar IDs de muestra como nombres de filas
rownames(gene_expression) <- classes$SampleID
##Añadir la clase como última columna
data <- gene_expression %>%
  mutate(Class = as.factor(classes[,2]))


```

## DEPURADO DE DATOS

```{r depuración_NAs}
##calcular la proporcion de NA por gen
na_prop <- colMeans(is.na(data))
##eliminar genes con mas del 20% de NA
data <- data[, na_prop < 0.2] #(no se elimina ningun gen)

#IMPUTACION DE VALORES NA
##No haria falta ya que no hay ningun NA, pero en un caso real cabria hacerlo
###Aqui se estima un valor razonable para los NA usando la informacion disponible
###Se realiza con KNN (k-nearest neighbors)
##separar datos y clase ya que nunca imputamos la clase, solo los datos
x <- data[, colnames(data) != "Class"]
y <- data$Class

```

## RECUENTO DE CEROS

```{r eliminadas_cero_filas}
# identifican filas a eliminar
zero_counts_filas <- rowSums(x == 0)
umbral_filas <- ncol(x) * 0.95  # Umbral del 95% de los genes

# Identifican muestras a eliminar
indices_filas_eliminar <- which(zero_counts_filas > umbral_filas)
nombres_muestras_eliminadas <- rownames(x)[indices_filas_eliminar]
num_filas_eliminadas <- length(indices_filas_eliminar)

cat("Muestras (filas) eliminadas (> 95% ceros):", num_filas_eliminadas, "\n")

```

```{r ceros_genes}
zero_counts <- colSums(x == 0)
umbral_95_por_ciento <- nrow(x) * 0.95 #umbral del 95%

# Se crea un df con el numero de ceros por variable
zero_df <- data.frame(
  Variable = names(zero_counts),
  Ceros = as.numeric(zero_counts)
)


# Total de filas para el porcentaje
total_filas <- nrow(x)

variables_criticas <- zero_df %>%
  filter(Ceros > umbral_95_por_ciento) %>%
  mutate(Porcentaje = (Ceros / total_filas) * 100) %>% # Crea la columna de porcentaje
  arrange(desc(Ceros))                                 # Ordena de mayor a menor

# Visualizar la tabla
print(variables_criticas)



```

```{r eliminadas_ceros}
# identifican variables a eliminar
nombres_variables_eliminadas_ceros <- variables_criticas$Variable
num_eliminadas <- length(nombres_variables_eliminadas_ceros)


cat("Variables (columnas) eliminadas (> 95% ceros):", num_eliminadas, "\n")
cat("Variables:", paste(nombres_variables_eliminadas_ceros, collapse = ", "), "\n")


x_sin_ceros <- x[, !(names(x) %in% nombres_variables_eliminadas_ceros)]
```

## DESVIACIÓN ESTANDAR

```{r sd}
# Variables con desviación estandar 0
# Columnas con SD = 0 que indican que son constantes

desviaciones <- apply(x_sin_ceros, 2, sd) # La función apply con margen 2 calcula la SD por columna.


columnas_a_mantener <- desviaciones > 0 # se mantienen las columnas que su desviación estándar es > 0


genes_eliminados <- sum(!columnas_a_mantener)
nombres_genes_eliminados <- colnames(x_sin_ceros)[!columnas_a_mantener]

cat("Genes (columnas) eliminados (SD = 0):", genes_eliminados, "\n")


# filtro sobre el dataset original para dejar solo con los genes útiles
x_filtrado <- x_sin_ceros[, columnas_a_mantener]

```

## NORMALIZACIÓN

```{r centrado y escalado}

##centrar y escalar
preproc_scale <- preProcess(x_filtrado, method = c("center", "scale"))
X_scaled <- predict(preproc_scale, x_filtrado)


##dataset final
data_final <- X_scaled
data_final$Class <- y

cat("Dimensiones del dataset final:", dim(data_final))
```

# 4. MÉTODOS NO SUPERVISADOS

## 4.1 Reducción de dimensionalidad

### 4.1.1 PCA

Se ha seleccionado PCA porque permite resumir y explorar la estructura de los datos en un conjunto de alta dimensión, como es este caso. Al mismo tiempo, el PCA es útil para explorar la estructura general de los datos, aunque no captura relaciones no lineales.

```{r PCA, include=TRUE}
# Cálculo de componentes principales (usamos los datos ya escalados)
pca.results <- prcomp(X_scaled, center = FALSE, scale. = FALSE)

# Resultado de las componentes principales en un dataframe
pca.df <- data.frame(pca.results$x)

# Análisis de la Varianza
varianzas <- pca.results$sdev^2
total.varianza <- sum(varianzas)
varianza.explicada <- varianzas / total.varianza
varianza.acumulada <- cumsum(varianza.explicada)

# Número de componentes que explican el 70%
n.pc <- min(which(varianza.acumulada > 0.70))
print(paste("Número de PCs para el 70% de varianza:", n.pc))


### Grafico PCA 2D

# Configuración de etiquetas dinámicas para los ejes
x_label <- paste0("PC1 (", round(varianza.explicada[1] * 100, 2), "%)")
y_label <- paste0("PC2 (", round(varianza.explicada[2] * 100, 2), "%)")

# Representación gráfica
# Usamos 'y' que definimos antes como el factor de las clases
ggplot(pca.df, aes(x = PC1, y = PC2, color = y)) + 
  geom_point(size = 3, alpha = 0.8) +
  scale_color_manual(values = c("#F8766D", "#A3A000", "#00B0F6", "#00BF7D", "#E76BF3")) +
  labs(title = 'PCA', 
      
       x = x_label, 
       y = y_label, 
       color = 'Grupo') +
  theme_classic() +
  theme(
    panel.grid.major = element_line(color = "gray90"), 
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "gray95"), 
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5)
  )


### Gráfico PCA 3D

# Definir la etiqueta del tercer eje (PC3)
z_label <- paste0('PC3 (', round(varianza.explicada[3]*100, 2), '%)')

#  Generar el gráfico interactivo 3D
plot_ly(pca.df, 
        x = ~PC1, 
        y = ~PC2, 
        z = ~PC3, 
        color = ~y,  # Usamos 'y' que contiene tus etiquetas de Class
        colors = c("#F8766D", "#A3A000", "#00B0F6", "#00BF7D", "#E76BF3"),
        type = 'scatter3d', 
        mode = 'markers',
        marker = list(size = 4, opacity = 0.8)) %>%
  layout(title = 'PCA 3D - Types of Cancer',
         scene = list(xaxis = list(title = x_label),
                      yaxis = list(title = y_label),
                      zaxis = list(title = z_label)))

```

### 4.1.2 UMAP

xxx

```{r UMAP, include=TRUE}

# Separar datos y etiquetas para UMAP
x_umap <- data_final[, colnames(data_final) != "Class"]
labels_umap <- data_final$Class

# Aplicar UMAP con parámetros ajustados
# n_neighbors: número de vecinos cercanos (20% del total de muestras)
# n_components: dimensiones de salida (2 para visualización)
# min_dist: distancia mínima entre puntos
umap_results <- umap(x_umap, 
                     n_neighbors = 0.2 * nrow(x_umap),
                     n_components = 2, 
                     min_dist = 0.1, 
                     local_connectivity = 1,
                     ret_model = TRUE, 
                     verbose = TRUE)

# Crear dataframe con resultados de UMAP
umap_df <- data.frame(umap_results$embedding)
colnames(umap_df) <- c("UMAP1", "UMAP2")

# Añadir las etiquetas para el gráfico
umap_df$Class <- labels_umap

# Visualizar resultados UMAP
ggplot(umap_df, aes(x = UMAP1, y = UMAP2, color = Class)) +
  geom_point(size = 3) +
  labs(title = "UMAP - Expresión Genética", 
       x = "UMAP1", 
       y = "UMAP2", 
       color = "Tipo") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"), 
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"), 
        plot.title = element_text(hjust = 0.5))
```

## 4.2 CLUSTERIZACIÓN

### 4.2.1 K-means

Se ha seleccionado K-means por su simplicidad y eficiencia para identificar clusters. Aunque no captura relaciones no lineales, se aplica so sobre el dataset original y sobre el espacio reducido por PCA, donde los grupos son más distinguibles.

```{r k-means_n}
## K-means

# n optimo de clusters
fviz_nbclust(X_scaled, kmeans, method = "wss") +
  ggtitle("Optimal number of clusters", subtitle = "") +
  theme_classic()

#A partir de $k = 4$, la curva se vuelve mucho más plana (lineal), lo que sugiere que añadir más grupos no está reduciendo la varianza interna de forma significativa.


### k-means 2D

# 1. Ejecutar K-means sobre los datos ESCALADOS
# Usamos k=5 como indicaste en tu ejemplo
set.seed(123) # Para que el resultado sea reproducible
kmeans.result <- kmeans(X_scaled, 
                        centers = 4, 
                        iter.max = 100, 
                        nstart = 25)

# 2. Visualización con fviz_cluster
# Esta función hace un PCA interno para poder mostrar los datos en 2D
fviz_cluster(kmeans.result, 
             data = X_scaled, 
             geom = "point",
             palette = c("#F8766D", "#A3A000", "#00B0F6", "#00BF7D"), 
             ellipse.type = "convex", # Encierra los grupos en polígonos
             ggtheme = theme_minimal(),
             main = "Clustering K-means (k=4)")


### K-means sobre PCA

fviz_nbclust(pca.df[,1:3], kmeans, method = "wss") +
  ggtitle("Optimal number of clusters", subtitle = "") +
  theme_classic()


#Similar al anterior, se seleccionan 4 componentes.

# 1. Ejecutar K-means sobre las primeras 3 dimensiones del PCA
# Esto agrupa las muestras según su posición en el espacio 3D visualizado

kmeans.pca <- kmeans(pca.df[, 1:3], 
                     centers = 4, 
                     nstart = 100, 
                     iter.max = 300)


# 2. Visualización 2D (Proyección de los clusters)
fviz_cluster(kmeans.pca, 
             data = pca.df[, 1:3], 
             geom = "point", 
             palette = c("#F8766D", "#A3A000", "#00B0F6", "#00BF7D"),
             ellipse.type = "convex",
             ggtheme = theme_minimal(),
             main = "K-means Clustering sobre PC1-PC3 (2D)")

# 3. Preparar los clusters para el gráfico 3D
clusters <- kmeans.pca$cluster

# 4. Visualización 3D Interactiva
plot_ly(
  x = pca.df[, 1],
  y = pca.df[, 2],
  z = pca.df[, 3],
  color = as.factor(clusters),
  colors = c("#F8766D", "#A3A000", "#00B0F6", "#00BF7D"),
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 4, opacity = 0.8)
) %>%
  layout(
    title = 'Clusters K-means en Espacio PCA (3D)',
    scene = list(
      xaxis = list(title = 'PC1'),
      yaxis = list(title = 'PC2'),
      zaxis = list(title = 'PC3')
    )
  )
```

### 4.2.2 HEATMAP

```{r HEATMAP}



# Calcular varianza de cada gen
varianzas <- apply(x_filtrado, 2, var)

# Seleccionar top genes más representativos

# Calculamos un ANOVA para cada gen respecto a la Clase
p_values_anova <- apply(x_filtrado, 2, function(gen) {
  res_anova <- aov(gen ~ y)
  summary(res_anova)[[1]][["Pr(>F)"]][1]
})

# Elegimos los 20 genes con el p-valor más bajo (los más significativos)
top_genes_anova <- names(sort(p_values_anova))[1:20]
matriz_anova <- x_filtrado[, top_genes_anova]


# Preparar anotaciones para mostrar las clases reales
annotation_col <- data.frame(
  Clase = y
)

rownames(annotation_col) <- rownames(matriz_anova)




colores_estilo_umap <- c("#F8766D", "#A3A000", "#00B0F6", "#00BF7D", "#E76BF3")

# Colores para las clases
ann_colors <- list(
  Clase = setNames(
    colores_estilo_umap,
    unique(y) 
  )
)


# Heatmap con clustering jerárquico (método Ward)
pheatmap(t(matriz_anova),  # Transponer: genes en filas, muestras en columnas
         scale = "none",
         clustering_method = "ward.D",
         clustering_distance_rows = "euclidean",
         clustering_distance_cols = "euclidean",
         annotation_col = annotation_col,
         annotation_colors = ann_colors,
         show_rownames = TRUE,
         show_colnames = FALSE,
         main = "Clustering Jerárquico Aglomerativo - Método Ward",
         fontsize = 10,
         border_color = NA)
```

# 5. MÉTODOS SUPERVISADOS

## 5.1 KNN

```{r KNN}

#IMPLEMENTACION DE KNN

# Se construye el dataset final para KNN:
# - Variables predictoras: genes escalados y filtrados (X_scaled)
# - Variable respuesta: Class

data_knn <- as.data.frame(X_scaled)
data_knn$Class <- y



# División en entrenamiento y test


set.seed(123)

# Se mantiene la proporción de clases
train_index <- createDataPartition(data_knn$Class,
                                   p = 0.8,
                                   list = FALSE)

train_data <- data_knn[train_index, ]
test_data  <- data_knn[-train_index, ]



# Entrenamiento del modelo KNN


# Se utiliza validación cruzada (10-fold CV)
# para seleccionar el valor óptimo de k

control <- trainControl(
  method = "cv",
  number = 10
)

# Se prueban distintos valores de k (solo impares)
grid_k <- expand.grid(
  k = seq(3, 21, by = 2)
)

set.seed(123)

knn_model <- train(
  Class ~ .,
  data = train_data,
  method = "knn",
  trControl = control,
  tuneGrid = grid_k
)

# Mostrar el mejor valor de k
knn_model

#El mejor valor de K que se ha obtenido ha sido o bien k = 9, k = 11, k = 13, pero caret elige 13 (primer máximo estable)


# Evaluación del modelo


# Predicciones sobre el conjunto de test
pred_knn <- predict(knn_model, newdata = test_data)

# Matriz de confusión y métricas de evaluación
cm <- confusionMatrix(pred_knn, test_data$Class)
cm

cm_df <- as.data.frame(cm$table)


ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Matriz de confusión del clasificador KNN",
       x = "Clase real",
       y = "Clase predicha") +
  theme_minimal()
```

## 5.2 SVM

```{r SVM}


# Preparar X (predictores) e y (clases)
X <- data_final[, colnames(data_final) != "Class"]
y <- as.factor(data_final$Class)

dim(X)
length(y)

# Cargar librería (partición estratificada + entrenamiento + evaluación)


# Eliminar predictores con varianza cero (evita warnings al centrar/escalar)
nzv <- nearZeroVar(X, saveMetrics = TRUE)
X <- X[, !nzv$zeroVar]

# Partición train/test estratificada
set.seed(123)
train_index <- createDataPartition(y, p = 0.80, list = FALSE)

X_train <- X[train_index, ]
X_test  <- X[-train_index, ]

y_train <- y[train_index]
y_test  <- y[-train_index]

prop.table(table(y_train))
prop.table(table(y_test))

# Control del entrenamiento: validación cruzada 10-fold
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE
)

# Entrenar el modelo SVM con kernel gaussiano (RBF) -> en caret: svmRadial
set.seed(123)
svm_model <- train(
  x = X_train,
  y = y_train,
  method = "svmRadial",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  metric = "Accuracy"
)

svm_model
plot(svm_model)  # rendimiento vs hiperparámetro (C)

# Predicción en test y matriz de confusión
pred_test <- predict(svm_model, newdata = X_test)
cm <- confusionMatrix(pred_test, y_test)
cm

# Extracción de métricas generales
acc   <- cm$overall["Accuracy"]
kappa <- cm$overall["Kappa"]

acc
kappa

# Métricas por clase (sensibilidad y especificidad)
sens <- cm$byClass[, "Sensitivity"]
spec <- cm$byClass[, "Specificity"]

sens
spec

# F1 por clase (cálculo manual: 2 * (precision * recall) / (precision + recall))
precision <- cm$byClass[, "Pos Pred Value"]
recall    <- cm$byClass[, "Sensitivity"]

f1 <- 2 * (precision * recall) / (precision + recall)
f1[is.nan(f1)] <- NA

f1

# F1 macro-promedio (útil en multiclase)
f1_macro <- mean(f1, na.rm = TRUE)
f1_macro




```
```{r}
#matriz de confusión a data frame
cm_df <- as.data.frame(cm$table)



ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 5) +
  scale_fill_gradient(low = "white", high = "firebrick") +
  labs(title = "Matriz de Confusión: Clasificador SVM",
       x = "Clase Real (Referencia)",
       y = "Clase Predicha") +
  theme_minimal()
```

## 5.3 RANDOM FOREST

```{r RF}
#APRENDIZAJE NO SUPERVISADO RANDOM FOREST

##division de datos (0.7 train y 0.3 test)
trainIndex <- createDataPartition(data_final$Class, p = 0.7, lis = FALSE)
train_data <- data_final[trainIndex, ]
test_data <- data_final[-trainIndex, ]

cat("Dimension de train_data:", dim(train_data), "\n")
cat("Dimension de test_data:", dim(test_data), "\n")
cat("Dimension de clases en train:\n")
print(table(train_data$Class))
cat("\nDistribucion de clases en test:\n")
print(table(test_data$Class))

#ENTRENAMIENTO DEL MODELO
##entrenar Random Forest con validacion cruzada
rf_model <- train(
  Class ~ .,
  data = train_data,
  method = "rf",
  trControl = trainControl(
    method = "cv", #validacion cruzada
    number = 5, #5 folds
    savePredictions = TRUE,
    classProbs = TRUE #para calcular curvas ROC
  ),
  ntree = 75, #numero de arboles
  importance = TRUE #calcular importancia de variables
)

print(rf_model)
cat("\nMejor valor de mtry:", rf_model$bestTune$mtry, "\n")

#PREDICCIONES
##predicciones en test_data
predictions <- predict(rf_model, test_data)
predictions
predictions_prob <- predict(rf_model, test_data, type = "prob")
predictions_prob

#METRICAS DE EVALUACION
##matriz de confusion
conf_matrix <- confusionMatrix(predictions, test_data$Class)
print(conf_matrix)

##extraer metricas clave
cat("Accuracy:", round(conf_matrix$overall['Accuracy'], 4), "\n")
cat("\nMétricas por clase:\n")
print(conf_matrix$byClass[, c('Sensitivity', 'Specificity', 'Precision', 'F1')])

#metricas promedio
cat("\nMétricas promedio (macro):\n")
cat("Sensitivity promedio:", round(mean(conf_matrix$byClass[,'Sensitivity'], na.rm=TRUE), 4), "\n")
cat("Specificity promedio:", round(mean(conf_matrix$byClass[,'Specificity'], na.rm=TRUE), 4), "\n")
cat("F1-Score promedio:", round(mean(conf_matrix$byClass[,'F1'], na.rm=TRUE), 4), "\n")

#IMPORTANCIA DE VARIABLES (GENES)
importance <- varImp(rf_model, scale = FALSE)
cat("\nTop 20 genes mas importances:\n")
print(head(importance$importance, 20))

##visualizacion de importancia
pdf("random_forest_importance.pdf", width = 10, height = 8)
plot(importance, top = 20, main = "Top 20 genes mas importances")
dev.off()
cat("\nGrafico guardado en: random_forest_importance.pdf\n")


#ANALISIS DE ERRORES
##Identificar muestras mal clasificadas
errors <- which(predictions != test_data$Class)
cat("Numero de muestras mal clasificadas:", length(errors), "\n")
cat("Porcentaje de error:", round(length(errors)/nrow(test_data)*100, 2), "%\n")

if(length(errors) > 0 && length(errors) <= 10) {
  cat("\nMuestras mal clasificadas:\n")
  error_df <- data.frame(
    Muestra = rownames(test_data)[errors],
    Clase_Real = test_data$Class[errors],
    Prediccion = predictions[errors]
  )
  print(error_df)
}
```
```{r matriz_confucion_rf}

cm_rf_df <- as.data.frame(conf_matrix$table)


ggplot(cm_rf_df, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 5) +
  scale_fill_gradient(low = "white", high = "darkgreen") + # Color verde para variar del SVM
  labs(title = "Matriz de Confusión: Random Forest",
       x = "Clase Real (Referencia)",
       y = "Clase Predicha") +
  theme_minimal()
```

# 6. RESPUESTA A LAS PREGUNTAS

## 6.1 Procesamiento de los datos

### 6.1.1 ¿Qué método habéis escogido para llevar a cabo la imputación de los datos?

En el flujo de preprocesamiento se evaluó primero la presencia de valores perdidos (NA) por gen. Tras aplicar el filtrado por proporción de NA (umbral \< 20%), se comprobó que en el conjunto de datos final no existían valores NA, por lo que no fue necesaria una imputación efectiva para continuar con los análisis.

Aun así, se consideró como método idóneo para un caso real la imputación mediante KNN (K-Nearest Neighbors), ya que en datos de expresión génica resulta ventajoso imputar utilizando la similitud multivariante entre muestras. Este enfoque preserva mejor las correlaciones y patrones de coexpresión que métodos más simples como media/mediana, que podrían distorsionar la estructura biológica del dataset.

### 6.1.2 ¿Habéis llevado a cabo algún otro tipo de procesamiento?

Sí. Se implementaron varios pasos adicionales importantes para mejorar la calidad del dataset y optimizar el rendimiento de los algoritmos:

Filtrado de muestras con exceso de ceros: se eliminaron muestras con más del 95% de genes con valor 0, ya que este patrón suele asociarse a baja calidad técnica o perfiles poco informativos.

Filtrado de genes con exceso de ceros: se eliminaron genes con más del 95% de ceros en las muestras, dado que aportan escasa señal biológica y tienden a introducir ruido en análisis posteriores.

Eliminación de genes constantes (SD = 0): se descartaron variables sin variabilidad, ya que no contribuyen a discriminar entre clases y además pueden generar problemas matemáticos en la normalización (divisiones por cero).

Normalización (centrado y escalado): se aplicó estandarización tipo z-score para asegurar que todos los genes contribuyen de forma comparable, especialmente en técnicas sensibles a escala como PCA, UMAP, KNN y SVM.

Estos pasos permiten reducir ruido, eliminar redundancia y hacer el conjunto de datos más estable para modelado. 
## 6.2 Métodos no supervisados

### 6.2.1 ¿Por qué se seleccionaron estas técnicas de reducción de dimensionalidad?

Se eligió **PCA** debido a la alta complejidad y dimensionalidad del dataset original (como demuestra el hecho de necesitar 43 componentes para alcanzar el 70% de la varianza). El objetivo principal era simplificar la estructura de los datos sin perder la información esencial, permitiendo identificar patrones globales y tendencias generales que a simple vista son invisibles. Además, se utilizó como paso previo para mejorar la eficiencia de otros algoritmos, eliminando el ruido y la redundancia entre variables correlacionadas.

Se seleccionó **UMAP** dada su excepcional capacidad para preservar tanto la estructura local como la global en datasets de alta dimensionalidad, permitiendo identificar no solo qué muestras son similares entre sí, sino también cómo se relacionan los grandes grupos biológicos. A diferencia de métodos lineales como el PCA, UMAP gestiona eficazmente las relaciones no lineales intrínsecas en los datos de expresión génica, 'comprimiendo' miles de dimensiones en un espacio bidimensional sin sacrificar la topología de los datos. Además, esta técnica mejora significativamente la separabilidad de las clases, facilitando la tarea posterior de algoritmos supervisados como las máquinas de vectores de soporte (SVM) al generar límites de decisión mucho más definidos y compactos.

### 6.2.2 ¿Por qué se seleccionaron estas técnicas de clusterización?

La elección de **K-means** responde a su equilibrio entre eficacia y coste computacional. Es una técnica ideal para establecer una base de segmentación rápida, especialmente útil cuando se trabaja tanto sobre el dataset completo como sobre las dimensiones proyectadas por el PCA. Se seleccionó por su capacidad para realizar una partición directa del espacio, facilitando la interpretación de los grupos formados en función de su cercanía a los centroides.

La elección de la **clusterización jerárquica aglomerativa (Heatmap)** responde a su capacidad para integrar en una sola visualización la estructura jerárquica de los datos y los perfiles de expresión génica. Es una técnica ideal para identificar patrones de co-expresión y firmas moleculares complejas, permitiendo explorar las relaciones de similitud en múltiples niveles sin la restricción de prefijar un número de grupos. Se seleccionó por su potencia para el análisis exploratorio bidimensional, facilitando la interpretación biológica de los clústeres al revelar visualmente qué genes específicos son responsables de la diferenciación entre las distintas clases de muestras.

### 6.2.3. Aspectos positivos y negativos de cada una

-   **PCA:**
    -   **Positivo:** Excelente para comprimir información y facilitar la visualización de grandes volúmenes de datos. Ayuda a evitar el *overfitting* al reducir el número de variables.
    -   **Negativo:** Tiene una naturaleza lineal, lo que significa que si existen relaciones complejas o curvas en los datos (no lineales), el PCA no podrá capturarlas adecuadamente.
-   **UMAP:**
    -   **Positivo:**
        -   Conservación de la topología: A diferencia de t-SNE, UMAP suele mantener mejor las distancias entre clústeres lejanos. Por ejemplo, en tu gráfico, la distancia de AGH al resto tiene un significado biológico real de diferenciación.
        -   Escalabilidad y Velocidad: Es computacionalmente más eficiente que otros algoritmos no lineales, lo que permite procesar grandes matrices de expresión génica rápidamente.
        -   Claridad Visual: Genera agrupamientos muy definidos, lo que ayuda a identificar subtipos celulares o estados patológicos de forma intuitiva.
    -   **Negativo:**
        -   Sensibilidad a Hiperparámetros: la elección de n_neighbors (vecinos) y min_dist (distancia mínima) cambia drásticamente el resultado.
        -   Un "n_neighbors" bajo se enfoca en detalles muy pequeños.
        -   Un "n_neighbors" alto prioriza la visión general.
        -   Interpretación de los ejes: Los ejes UMAP1 y UMAP2 no tienen una unidad física ni una magnitud biológica directa (como sí la tienen los Componentes Principales en un PCA). Son unidades abstractas.
        -   Naturaleza Estocástica: Si no se fija una "semilla" (seed), el gráfico puede variar ligeramente cada vez que se ejecuta, lo que puede afectar la reproducibilidad si no se es cuidadoso.
-   **K-means:**
    -   **Positivo:** Es extremadamente rápido y escalable. Su implementación es intuitiva y permite asignar etiquetas de grupo de manera inmediata a cada observación.

    -   **Negativo:** Es muy vulnerable a los valores atípicos y obliga a definir de antemano el número de clústeres , lo cual no siempre es evidente. Además, asume que los grupos tienen formas esféricas, lo que no siempre ocurre en la realidad.
-   **Heatmap**
    -   **Positivos:**
        -   Jerarquía visual: Gracias al dendrograma, podemos ver no solo que dos muestras son parecidas, sino qué tan parecidas son en relación con el resto del dataset sin prefijar el número de grupos (a diferencia de K-means).
        -   Robustez: Como bien dicen tus apuntes, Ward es más resistente a valores atípicos (outliers) porque su métrica se basa en la suma de cuadrados, lo que suaviza el impacto de ruidos puntuales en la medición de un gen.
        -   Interpretación Dual: Es la única técnica que permite ver el "porqué" de un clúster: podemos señalar exactamente qué genes están "en rojo" (sobreexpresados) para justificar la agrupación de una muestra.
    -   **Negativos:**
        -   Coste Computacional: Al calcular una matriz de distancias completa (N×N), si el dataset fuera de millones de muestras en lugar de 801, el ordenador se quedaría sin memoria. \*Inestabilidad (Sensibilidad): Si eliminamos algunas muestras de una clase (ej. 10 muestras de BRCA), el árbol jerárquico podría reestructurarse de forma distinta, lo que complica la replicabilidad exacta en otros estudios.
        -   Subjetividad: La elección de la distancia (Euclídea) y el linkage (Ward) es una decisión del investigador; otro analista podría usar distancia Manhattan y obtener grupos diferentes.

### 6.2.4. En la clusterización, ¿podéis afirmar con certeza que los clústeres generados son los mejores posibles?

No se puede afirmar con certeza absoluta que los clústeres generados sean los mejores posibles, ya que la clusterización es un proceso de aprendizaje no supervisado donde el resultado óptimo es dependiente de la métrica y el algoritmo utilizado. Esta incertidumbre se confirma al contrastar los resultados de K-means con la clusterización jerárquica del Heatmap, donde se observa un solapamiento significativo y una falta de fronteras nítidas entre grupos específicos (como CFB, CGC, CHC y HPB), indicando que sus perfiles de expresión son altamente similares. Mientras que K-means puede converger en mínimos locales debido a la disposición inicial de los centroides, la estructura jerárquica del Heatmap revela que, aunque existen tendencias globales claras, la alta dimensionalidad y la naturaleza no lineal de los datos sugieren la existencia de agrupaciones más óptimas que las métricas de distancia tradicionales no logran capturar con total precisión.

## 6.3 Métodos supervisados

### 6.3.1 ¿Cuál es el motivo por el cual habéis seleccionado ambas técnicas de aprendizaje supervisado?

Se seleccionaron KNN, SVM y Random Forest por ser modelos adecuados y complementarios para clasificación con datos de expresión génica:

KNN permite comparar muestras por proximidad en el espacio de genes. Es sencillo y útil como baseline, aunque requiere escalado y puede verse afectado por alta dimensionalidad.

SVM (kernel radial/RBF) es especialmente eficaz en alta dimensionalidad y permite fronteras de decisión no lineales, lo que encaja muy bien con perfiles genómicos complejos.

Random Forest ofrece alta robustez frente a ruido, suele generalizar bien y además permite extraer importancia de variables (genes), aportando valor interpretativo. 

### 6.3.2 ¿Cuál ha dado mejores resultados a la hora de clasificar las muestras? 

Tras analizar los resultados de las matrices de confusión, el modelo KNN es el que presenta el mejor desempeño, logrando una exactitud (Accuracy) del 100% y un índice Kappa de 1.0. Este modelo clasificó correctamente la totalidad de las muestras, mostrando una sensibilidad y especificidad perfectas en todas las categorías.

En segundo lugar, el modelo Random Forest (RF) mostró también un buen rendimiento con una precisión del 97.48%, cometiendo únicamente errores marginales en la distinción de las clases CGC y HPB. Por el contrario, el modelo SVM resultó ser el menos eficaz, aunque su precisión global es del 85.53%, presenta un fallo crítico en la clase CHC, donde su sensibilidad cae drásticamente al 14.8%, lo que indica una incapacidad casi total para identificar correctamente dicha categoría en comparación con los otros dos algoritmos.

### 6.3.3 ¿Habéis considerado oportuno implementar algún método de reducción de dimensionalidad para procesar los datos antes de implementarlos en dichas técnicas? ¿Por qué?

Sí, se consideró la posibilidad de aplicar métodos de reducción de dimensionalidad como PCA o UMAP antes del entrenamiento de los modelos supervisados, debido a la alta dimensionalidad del dataset de expresión génica. Sin embargo, se decidió no aplicar estas técnicas en el bloque supervisado, y trabajar directamente sobre el espacio original de genes tras un preprocesamiento riguroso.

Los modelos empleados (SVM con kernel radial y Random Forest) están específicamente diseñados para manejar espacios de alta dimensionalidad y relaciones no lineales, por lo que no requieren necesariamente una reducción previa de dimensiones para funcionar de manera eficaz. Además, en el caso de Random Forest, mantener los genes originales es fundamental para poder interpretar la importancia de las variables y extraer conclusiones biológicas.

Por último, los métodos de reducción de dimensionalidad se reservaron para el bloque no supervisado con un objetivo principalmente exploratorio y de visualización (PCA y UMAP), mientras que en el aprendizaje supervisado se priorizó la preservación de la información original para maximizar la capacidad predictiva y la interpretabilidad de los modelos.

### 6.3.4 ¿Qué aspectos positivos y negativos tienen cada una de las técnicas que habéis escogido?

Cada técnica supervisada presenta ventajas y limitaciones en este problema de clasificación con datos de expresión génica. En primer lugar, el clasificador KNN destaca por su sencillez y por ser un método intuitivo, ya que basa la predicción en la similitud con las muestras más cercanas. Esto puede ser útil cuando las clases presentan patrones de vecindad claros. Sin embargo, su rendimiento puede verse afectado por la alta dimensionalidad del dataset (maldición de la dimensionalidad), además de ser sensible a la escala de las variables (por lo que requiere normalización) y a la elección del parámetro k, que condiciona notablemente el resultado final.

Por otro lado, las máquinas de vectores de soporte (SVM), especialmente con kernel radial, son muy adecuadas para datos con gran número de variables como los de expresión génica. Este método puede generar fronteras de decisión no lineales, lo que permite capturar patrones complejos y generalmente proporciona un buen rendimiento y capacidad de generalización. Como limitación, SVM suele requerir un ajuste cuidadoso de hiperparámetros y puede tener un mayor coste computacional, especialmente cuando se trabaja con conjuntos de datos de alta dimensión.

Finalmente, Random Forest es un modelo robusto y flexible, capaz de manejar relaciones no lineales y de generalizar bien al combinar múltiples árboles de decisión. Una ventaja muy relevante en el contexto biológico es que permite obtener estimaciones de importancia de variables, lo que facilita identificar genes que contribuyen más a la clasificación. Como inconveniente, su interpretabilidad global es inferior a modelos más simples, y el rendimiento puede depender del ajuste de parámetros como el número de árboles o el número de variables evaluadas en cada partición.

## 6.4 De estas cuatro opciones, ¿qué tipo de arquitectura de deep learning sería la más adecuada para procesar datos de expresión génica?

En el caso de los datos de expresión génica utilizados en esta práctica, donde cada muestra se representa como un vector de valores numéricos correspondiente a la expresión de distintos genes, la arquitectura de deep learning más adecuada sería una red neuronal multicapa tipo perceptrón (MLP). Esto se debe a que el dataset tiene una estructura tabular, sin organización espacial como en imágenes ni dependencias secuenciales explícitas como en series temporales. Por tanto, una red MLP se adapta de forma directa a este tipo de entradas y permite modelar relaciones complejas entre genes mediante capas densas.

En comparación, las redes convolucionales suelen ser más apropiadas cuando existe una estructura espacial local (como sucede en imágenes), las redes recurrentes se utilizan principalmente para datos secuenciales o temporales, y las redes de grafos serían especialmente útiles si los datos estuvieran formulados explícitamente como una red biológica (por ejemplo, una red regulatoria gen-gén), lo cual no corresponde con la representación proporcionada en esta práctica.

```         
a)  Red de perceptrones (multiperceptron layers).
```

La arquitectura más adecuada para procesar datos de expresión génica en este caso sería una red de perceptrones multicapa (MLP), porque este tipo de datos se representa como una tabla de valores numéricos donde cada muestra es un vector de expresión (genes como variables). Es decir, no existe estructura espacial como en imágenes (donde tendría sentido una CNN), ni una dependencia temporal/secuencial clara como en textos o series temporales (donde se usarían RNN). Por ello, una MLP es el enfoque más natural y directo para aprender patrones complejos a partir de variables tabulares de alta dimensionalidad como los genes.

```         
    b)  Redes convolucionales.

 

    c)  Redes recurrentes.


    d)  Redes de grafos.
    
```         
